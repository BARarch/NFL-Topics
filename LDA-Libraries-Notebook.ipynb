{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Do an item query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import config as config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn = config.connect()\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import teamQuery as tq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "411"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tq.teamQueryCounts(\"Redskins\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "red = tq.teamQueryTexts('Redskins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corp = [text[3] for text in red]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "411"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(corp[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5167"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corp[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Step 2: Gensim 3.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\anthony\\scripts\\nfl-topics\\venv\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'wiki_en_wordids.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-736183642b43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mid2word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'wiki_en_wordids.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\anthony\\scripts\\nfl-topics\\venv\\lib\\site-packages\\gensim\\corpora\\dictionary.py\u001b[0m in \u001b[0;36mload_from_text\u001b[1;34m(fname)\u001b[0m\n\u001b[0;32m    389\u001b[0m         \"\"\"\n\u001b[0;32m    390\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 391\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    392\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mlineno\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m                 \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\anthony\\scripts\\nfl-topics\\venv\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[1;34m(uri, mode, **kw)\u001b[0m\n\u001b[0;32m    174\u001b[0m             \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m             \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'errors'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDEFAULT_ERRORS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfile_smart_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mparsed_uri\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscheme\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"s3\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"s3n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m's3u'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0ms3_open_uri\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\anthony\\scripts\\nfl-topics\\venv\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36mfile_smart_open\u001b[1;34m(fname, mode, encoding, errors)\u001b[0m\n\u001b[0;32m    669\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[0mraw_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 671\u001b[1;33m     \u001b[0mraw_fobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    672\u001b[0m     \u001b[0mdecompressed_fobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompression_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_fobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m     \u001b[0mdecoded_fobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoding_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecompressed_fobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'wiki_en_wordids.txt'"
     ]
    }
   ],
   "source": [
    "id2word = gensim.corpora.Dictionary.load_from_text('wiki_en_wordids.txt') ## Lets obtain this dictionary from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Graphlab with special API instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphlab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-6b5df377dfdc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgraphlab\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgl\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'graphlab'"
     ]
    }
   ],
   "source": [
    "import graphlab as gl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: lda 1.0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = lda.datasets.load_reuters() ## Don't know what this is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "395"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 ... 0 0 0]\n",
      " [7 0 2 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [1 0 1 ... 0 0 0]\n",
      " [1 0 1 ... 0 0 0]\n",
      " [1 0 1 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = lda.LDA(n_topics=20, n_iter=1500, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 395\n",
      "INFO:lda:vocab_size: 4258\n",
      "INFO:lda:n_words: 84010\n",
      "INFO:lda:n_topics: 20\n",
      "INFO:lda:n_iter: 1500\n",
      "INFO:lda:<0> log likelihood: -1051748\n",
      "INFO:lda:<10> log likelihood: -719800\n",
      "INFO:lda:<20> log likelihood: -699115\n",
      "INFO:lda:<30> log likelihood: -689370\n",
      "INFO:lda:<40> log likelihood: -684918\n",
      "INFO:lda:<50> log likelihood: -681322\n",
      "INFO:lda:<60> log likelihood: -678979\n",
      "INFO:lda:<70> log likelihood: -676598\n",
      "INFO:lda:<80> log likelihood: -675383\n",
      "INFO:lda:<90> log likelihood: -673316\n",
      "INFO:lda:<100> log likelihood: -672761\n",
      "INFO:lda:<110> log likelihood: -671320\n",
      "INFO:lda:<120> log likelihood: -669744\n",
      "INFO:lda:<130> log likelihood: -669292\n",
      "INFO:lda:<140> log likelihood: -667940\n",
      "INFO:lda:<150> log likelihood: -668038\n",
      "INFO:lda:<160> log likelihood: -667429\n",
      "INFO:lda:<170> log likelihood: -666475\n",
      "INFO:lda:<180> log likelihood: -665562\n",
      "INFO:lda:<190> log likelihood: -664920\n",
      "INFO:lda:<200> log likelihood: -664979\n",
      "INFO:lda:<210> log likelihood: -664722\n",
      "INFO:lda:<220> log likelihood: -664459\n",
      "INFO:lda:<230> log likelihood: -664360\n",
      "INFO:lda:<240> log likelihood: -663600\n",
      "INFO:lda:<250> log likelihood: -664164\n",
      "INFO:lda:<260> log likelihood: -663826\n",
      "INFO:lda:<270> log likelihood: -663458\n",
      "INFO:lda:<280> log likelihood: -663393\n",
      "INFO:lda:<290> log likelihood: -662904\n",
      "INFO:lda:<300> log likelihood: -662294\n",
      "INFO:lda:<310> log likelihood: -662031\n",
      "INFO:lda:<320> log likelihood: -662430\n",
      "INFO:lda:<330> log likelihood: -661601\n",
      "INFO:lda:<340> log likelihood: -662108\n",
      "INFO:lda:<350> log likelihood: -662152\n",
      "INFO:lda:<360> log likelihood: -661899\n",
      "INFO:lda:<370> log likelihood: -661012\n",
      "INFO:lda:<380> log likelihood: -661278\n",
      "INFO:lda:<390> log likelihood: -661085\n",
      "INFO:lda:<400> log likelihood: -660418\n",
      "INFO:lda:<410> log likelihood: -660510\n",
      "INFO:lda:<420> log likelihood: -660343\n",
      "INFO:lda:<430> log likelihood: -659789\n",
      "INFO:lda:<440> log likelihood: -659336\n",
      "INFO:lda:<450> log likelihood: -659039\n",
      "INFO:lda:<460> log likelihood: -659329\n",
      "INFO:lda:<470> log likelihood: -658707\n",
      "INFO:lda:<480> log likelihood: -658879\n",
      "INFO:lda:<490> log likelihood: -658819\n",
      "INFO:lda:<500> log likelihood: -658407\n",
      "INFO:lda:<510> log likelihood: -658651\n",
      "INFO:lda:<520> log likelihood: -658111\n",
      "INFO:lda:<530> log likelihood: -658018\n",
      "INFO:lda:<540> log likelihood: -658111\n",
      "INFO:lda:<550> log likelihood: -657925\n",
      "INFO:lda:<560> log likelihood: -657860\n",
      "INFO:lda:<570> log likelihood: -657494\n",
      "INFO:lda:<580> log likelihood: -657723\n",
      "INFO:lda:<590> log likelihood: -657591\n",
      "INFO:lda:<600> log likelihood: -657557\n",
      "INFO:lda:<610> log likelihood: -657505\n",
      "INFO:lda:<620> log likelihood: -657730\n",
      "INFO:lda:<630> log likelihood: -657304\n",
      "INFO:lda:<640> log likelihood: -657208\n",
      "INFO:lda:<650> log likelihood: -657518\n",
      "INFO:lda:<660> log likelihood: -657541\n",
      "INFO:lda:<670> log likelihood: -657381\n",
      "INFO:lda:<680> log likelihood: -657575\n",
      "INFO:lda:<690> log likelihood: -656985\n",
      "INFO:lda:<700> log likelihood: -656815\n",
      "INFO:lda:<710> log likelihood: -656930\n",
      "INFO:lda:<720> log likelihood: -656538\n",
      "INFO:lda:<730> log likelihood: -656291\n",
      "INFO:lda:<740> log likelihood: -656417\n",
      "INFO:lda:<750> log likelihood: -656747\n",
      "INFO:lda:<760> log likelihood: -656600\n",
      "INFO:lda:<770> log likelihood: -656269\n",
      "INFO:lda:<780> log likelihood: -656311\n",
      "INFO:lda:<790> log likelihood: -656069\n",
      "INFO:lda:<800> log likelihood: -656228\n",
      "INFO:lda:<810> log likelihood: -656178\n",
      "INFO:lda:<820> log likelihood: -655694\n",
      "INFO:lda:<830> log likelihood: -655997\n",
      "INFO:lda:<840> log likelihood: -656224\n",
      "INFO:lda:<850> log likelihood: -656197\n",
      "INFO:lda:<860> log likelihood: -655889\n",
      "INFO:lda:<870> log likelihood: -656180\n",
      "INFO:lda:<880> log likelihood: -656997\n",
      "INFO:lda:<890> log likelihood: -655989\n",
      "INFO:lda:<900> log likelihood: -655615\n",
      "INFO:lda:<910> log likelihood: -655584\n",
      "INFO:lda:<920> log likelihood: -656602\n",
      "INFO:lda:<930> log likelihood: -656083\n",
      "INFO:lda:<940> log likelihood: -656294\n",
      "INFO:lda:<950> log likelihood: -656257\n",
      "INFO:lda:<960> log likelihood: -656243\n",
      "INFO:lda:<970> log likelihood: -656028\n",
      "INFO:lda:<980> log likelihood: -655603\n",
      "INFO:lda:<990> log likelihood: -656012\n",
      "INFO:lda:<1000> log likelihood: -655849\n",
      "INFO:lda:<1010> log likelihood: -655376\n",
      "INFO:lda:<1020> log likelihood: -655417\n",
      "INFO:lda:<1030> log likelihood: -655856\n",
      "INFO:lda:<1040> log likelihood: -655197\n",
      "INFO:lda:<1050> log likelihood: -655938\n",
      "INFO:lda:<1060> log likelihood: -655529\n",
      "INFO:lda:<1070> log likelihood: -655092\n",
      "INFO:lda:<1080> log likelihood: -655119\n",
      "INFO:lda:<1090> log likelihood: -656215\n",
      "INFO:lda:<1100> log likelihood: -655602\n",
      "INFO:lda:<1110> log likelihood: -655296\n",
      "INFO:lda:<1120> log likelihood: -655547\n",
      "INFO:lda:<1130> log likelihood: -655580\n",
      "INFO:lda:<1140> log likelihood: -655604\n",
      "INFO:lda:<1150> log likelihood: -655168\n",
      "INFO:lda:<1160> log likelihood: -655281\n",
      "INFO:lda:<1170> log likelihood: -655409\n",
      "INFO:lda:<1180> log likelihood: -655517\n",
      "INFO:lda:<1190> log likelihood: -654922\n",
      "INFO:lda:<1200> log likelihood: -655304\n",
      "INFO:lda:<1210> log likelihood: -655852\n",
      "INFO:lda:<1220> log likelihood: -655184\n",
      "INFO:lda:<1230> log likelihood: -655650\n",
      "INFO:lda:<1240> log likelihood: -655606\n",
      "INFO:lda:<1250> log likelihood: -656086\n",
      "INFO:lda:<1260> log likelihood: -655698\n",
      "INFO:lda:<1270> log likelihood: -655351\n",
      "INFO:lda:<1280> log likelihood: -655686\n",
      "INFO:lda:<1290> log likelihood: -654801\n",
      "INFO:lda:<1300> log likelihood: -654973\n",
      "INFO:lda:<1310> log likelihood: -655186\n",
      "INFO:lda:<1320> log likelihood: -655128\n",
      "INFO:lda:<1330> log likelihood: -655365\n",
      "INFO:lda:<1340> log likelihood: -655338\n",
      "INFO:lda:<1350> log likelihood: -655219\n",
      "INFO:lda:<1360> log likelihood: -655115\n",
      "INFO:lda:<1370> log likelihood: -654930\n",
      "INFO:lda:<1380> log likelihood: -655209\n",
      "INFO:lda:<1390> log likelihood: -654940\n",
      "INFO:lda:<1400> log likelihood: -655055\n",
      "INFO:lda:<1410> log likelihood: -655286\n",
      "INFO:lda:<1420> log likelihood: -655316\n",
      "INFO:lda:<1430> log likelihood: -655257\n",
      "INFO:lda:<1440> log likelihood: -654964\n",
      "INFO:lda:<1450> log likelihood: -654884\n",
      "INFO:lda:<1460> log likelihood: -655493\n",
      "INFO:lda:<1470> log likelihood: -655415\n",
      "INFO:lda:<1480> log likelihood: -655192\n",
      "INFO:lda:<1490> log likelihood: -655728\n",
      "INFO:lda:<1499> log likelihood: -655858\n"
     ]
    }
   ],
   "source": [
    "z = model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lda.lda.LDA"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(z)   ## Success to this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lda.lda.LDA at 0x2a29400b8d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Scikit-Learn 0.19.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, ..., 0, 0, 0],\n",
       "       [7, 0, 2, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 1, ..., 0, 0, 0],\n",
       "       [1, 0, 1, ..., 0, 0, 0],\n",
       "       [1, 0, 1, ..., 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_topics=20, max_iter=5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\anthony\\scripts\\nfl-topics\\venv\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "c:\\users\\anthony\\scripts\\nfl-topics\\venv\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7, learning_method=None,\n",
       "             learning_offset=10.0, max_doc_update_iter=100, max_iter=5,\n",
       "             mean_change_tol=0.001, n_components=10, n_jobs=1, n_topics=20,\n",
       "             perp_tol=0.1, random_state=0, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7, learning_method=None,\n",
       "             learning_offset=10.0, max_doc_update_iter=100, max_iter=5,\n",
       "             mean_change_tol=0.001, n_components=10, n_jobs=1, n_topics=20,\n",
       "             perp_tol=0.1, random_state=0, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "INFO:sklearn.datasets.twenty_newsgroups:Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n",
      "INFO:sklearn.datasets.twenty_newsgroups:Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "## Pipline Example\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "#Initialize Variables\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_topics = 10\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_samples = dataset.data[:n_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Setup a tf vectorizer\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95,\n",
    "                                min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(data_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_topics=20,\n",
    "                               max_iter=5,\n",
    "                               learning_method='online',\n",
    "                               learning_offset=50.,\n",
    "                               random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2000x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 51752 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\anthony\\scripts\\nfl-topics\\venv\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='online', learning_offset=50.0,\n",
       "             max_doc_update_iter=100, max_iter=5, mean_change_tol=0.001,\n",
       "             n_components=10, n_jobs=1, n_topics=20, perp_tol=0.1,\n",
       "             random_state=0, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.fit(tf)  ## Success to this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: NMF and LDA from scikit-learn docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_topics = 10\n",
    "n_top_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nice Tokenizers\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "\n",
    "base_tokenizer = CountVectorizer().build_tokenizer()\n",
    "stemmer = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokenize = None\n",
    "tokenize = lambda text: [stemmer.stem(item) for item in base_tokenizer(text)]\n",
    "#tokenize = lambda text: [lemmatizer.lemmatize(item) for item in base_tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Anthony\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what', 'the', 'hell', 'is', 'this', 'yell', 'yeller', 'yell']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize('what the hell is this yell yeller yells')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 3s.\n"
     ]
    }
   ],
   "source": [
    "# Load the 20 newsgroups dataset and vectorize it.  \n",
    "# Use a few heuristics to filter out useless terms early on, the posts are stripped of headers,\n",
    "# footers and quotes replies, and common english words, words occuring in only one document\n",
    "# or in at least 95% of the documents re removed.\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "data_samples = dataset.data[:n_samples]\n",
    "print(\"done in %0.fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features for NMF...\n",
      "done in 13.296s.\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Vectorization\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=.95,\n",
    "                                   min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   ngram_range=(1,3),\n",
    "                                   tokenizer=tokenize,\n",
    "                                   stop_words='english')\n",
    "\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featuresIdf = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(featuresIdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "done in 3.054s.\n"
     ]
    }
   ],
   "source": [
    "# TF Vectorization - raw term counts\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=.95,\n",
    "                                min_df=2,\n",
    "                                ngram_range=(1,3),\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print('done in %0.3fs.' % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 1000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 1000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The print top words function\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NMF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the NMF model with tf-idf freatures,  n_samples=2000 and n_features=1000...\n",
      "done in 0.266s.\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting the NMF model with tf-idf freatures,  \"\n",
    "      'n_samples=%d and n_features=%d...'\n",
    "      % (n_samples, n_features))\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "nmf = NMF(n_components=n_topics,\n",
    "          random_state=1,\n",
    "          alpha=.1,\n",
    "          l1_ratio=.5)\n",
    "\n",
    "nmfResult = nmf.fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 1000)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1000)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics in NMF model:\n",
      "Topic #0: like just don think peopl use time make know ani\n",
      "Topic #1: window use program dos applic driver run os manag work\n",
      "Topic #2: god christian jesus bibl faith believ sin religion christ exist\n",
      "Topic #3: thank anyon pleas mail ani doe know advanc doe anyon thank advanc\n",
      "Topic #4: game team play win player season year score blue defens\n",
      "Topic #5: drive disk floppi hard card problem mac rom control softwar\n",
      "Topic #6: edu bank soon internet univers mit edu articl com send ftp\n",
      "Topic #7: key chip encrypt clipper govern use secur phone public law enforc\n",
      "Topic #8: file problem format ftp pub site sound avail convert help\n",
      "Topic #9: car mile tire buy price model brake insur new dealer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Topics in NMF model:\")\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NMF Model with generalized Kullback-Leibler divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the NMF model (generalized Kullback-Leibler divergence) with tf-idf freatures,  n_samples=2000 and n_features=1000...\n",
      "done in 4.920s.\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with tf-idf freatures,  \"\n",
    "      'n_samples=%d and n_features=%d...'\n",
    "      % (n_samples, n_features))\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "nmf = NMF(n_components=n_topics,\n",
    "          random_state=1,\n",
    "          beta_loss='kullback-leibler',\n",
    "          solver='mu',\n",
    "          max_iter=1000,\n",
    "          alpha=.1,\n",
    "          l1_ratio=.5)\n",
    "\n",
    "nmfResult = nmf.fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics in NMF model:\n",
      "Topic #0: like use onli make time peopl just way right veri\n",
      "Topic #1: use window thank ani help file pleas run look need\n",
      "Topic #2: god say doe question peopl know true refer ani whi\n",
      "Topic #3: thank anyon pleas post know like mail want someon just\n",
      "Topic #4: time year team win game ve play onli player didn\n",
      "Topic #5: drive onli work power need hard new time sale thing\n",
      "Topic #6: edu univers includ com space write articl group year internet\n",
      "Topic #7: use number data key chip doe world bit receiv phone\n",
      "Topic #8: think don like know whi say tri just wrong need\n",
      "Topic #9: good just veri want tri work use realli new look\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Topics in NMF model:\")\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the LDA model  with tf freatures,  n_samples=2000 and n_features=1000...\n",
      "done in 7.212s.\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting the LDA model  with tf freatures,  \"\n",
    "      'n_samples=%d and n_features=%d...'\n",
    "      % (n_samples, n_features))\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=n_topics,\n",
    "                                max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0,\n",
    "                                )\n",
    "\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics in LDA model:\n",
      "Topic #0: space 10 earth 12 moon surface probe 93 00 lunar\n",
      "Topic #1: game scsi play team flyers season games cubs color points\n",
      "Topic #2: people just don think like know god time say make\n",
      "Topic #3: thanks know like does bike don help interested advance looking\n",
      "Topic #4: section military division win weapon firearm dangerous license shall application\n",
      "Topic #5: edu windows com file mail graphics version software send ftp\n",
      "Topic #6: just car like new good don year think better cars\n",
      "Topic #7: 55 11 10 18 15 17 25 26 22 23\n",
      "Topic #8: health hiv new national president aids information disease pittsburgh united\n",
      "Topic #9: drive disk card hard drives speed power controller hard disk rom\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Topics in LDA model:\")\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NFL-Topics",
   "language": "python",
   "name": "nfl-topics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
