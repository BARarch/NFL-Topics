{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Do an item query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import config as config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn = config.connect()\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import teamQuery as tq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "411"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tq.teamQueryCounts(\"Redskins\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "red = tq.teamQueryTexts('Redskins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corp = [text[3] for text in red]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "411"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(corp[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5167"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corp[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Step 2: Gensim 3.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\anthony\\scripts\\nfl-topics\\venv\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'wiki_en_wordids.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-736183642b43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mid2word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'wiki_en_wordids.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\anthony\\scripts\\nfl-topics\\venv\\lib\\site-packages\\gensim\\corpora\\dictionary.py\u001b[0m in \u001b[0;36mload_from_text\u001b[1;34m(fname)\u001b[0m\n\u001b[0;32m    389\u001b[0m         \"\"\"\n\u001b[0;32m    390\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 391\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    392\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mlineno\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m                 \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\anthony\\scripts\\nfl-topics\\venv\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[1;34m(uri, mode, **kw)\u001b[0m\n\u001b[0;32m    174\u001b[0m             \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m             \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'errors'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDEFAULT_ERRORS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfile_smart_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mparsed_uri\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscheme\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"s3\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"s3n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m's3u'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0ms3_open_uri\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\anthony\\scripts\\nfl-topics\\venv\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36mfile_smart_open\u001b[1;34m(fname, mode, encoding, errors)\u001b[0m\n\u001b[0;32m    669\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[0mraw_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 671\u001b[1;33m     \u001b[0mraw_fobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    672\u001b[0m     \u001b[0mdecompressed_fobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompression_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_fobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m     \u001b[0mdecoded_fobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoding_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecompressed_fobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'wiki_en_wordids.txt'"
     ]
    }
   ],
   "source": [
    "id2word = gensim.corpora.Dictionary.load_from_text('wiki_en_wordids.txt') ## Lets obtain this dictionary from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Graphlab with special API instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphlab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-6b5df377dfdc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgraphlab\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgl\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'graphlab'"
     ]
    }
   ],
   "source": [
    "import graphlab as gl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: lda 1.0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = lda.datasets.load_reuters() ## Don't know what this is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "395"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 ... 0 0 0]\n",
      " [7 0 2 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [1 0 1 ... 0 0 0]\n",
      " [1 0 1 ... 0 0 0]\n",
      " [1 0 1 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = lda.LDA(n_topics=20, n_iter=1500, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 395\n",
      "INFO:lda:vocab_size: 4258\n",
      "INFO:lda:n_words: 84010\n",
      "INFO:lda:n_topics: 20\n",
      "INFO:lda:n_iter: 1500\n",
      "INFO:lda:<0> log likelihood: -1051748\n",
      "INFO:lda:<10> log likelihood: -719800\n",
      "INFO:lda:<20> log likelihood: -699115\n",
      "INFO:lda:<30> log likelihood: -689370\n",
      "INFO:lda:<40> log likelihood: -684918\n",
      "INFO:lda:<50> log likelihood: -681322\n",
      "INFO:lda:<60> log likelihood: -678979\n",
      "INFO:lda:<70> log likelihood: -676598\n",
      "INFO:lda:<80> log likelihood: -675383\n",
      "INFO:lda:<90> log likelihood: -673316\n",
      "INFO:lda:<100> log likelihood: -672761\n",
      "INFO:lda:<110> log likelihood: -671320\n",
      "INFO:lda:<120> log likelihood: -669744\n",
      "INFO:lda:<130> log likelihood: -669292\n",
      "INFO:lda:<140> log likelihood: -667940\n",
      "INFO:lda:<150> log likelihood: -668038\n",
      "INFO:lda:<160> log likelihood: -667429\n",
      "INFO:lda:<170> log likelihood: -666475\n",
      "INFO:lda:<180> log likelihood: -665562\n",
      "INFO:lda:<190> log likelihood: -664920\n",
      "INFO:lda:<200> log likelihood: -664979\n",
      "INFO:lda:<210> log likelihood: -664722\n",
      "INFO:lda:<220> log likelihood: -664459\n",
      "INFO:lda:<230> log likelihood: -664360\n",
      "INFO:lda:<240> log likelihood: -663600\n",
      "INFO:lda:<250> log likelihood: -664164\n",
      "INFO:lda:<260> log likelihood: -663826\n",
      "INFO:lda:<270> log likelihood: -663458\n",
      "INFO:lda:<280> log likelihood: -663393\n",
      "INFO:lda:<290> log likelihood: -662904\n",
      "INFO:lda:<300> log likelihood: -662294\n",
      "INFO:lda:<310> log likelihood: -662031\n",
      "INFO:lda:<320> log likelihood: -662430\n",
      "INFO:lda:<330> log likelihood: -661601\n",
      "INFO:lda:<340> log likelihood: -662108\n",
      "INFO:lda:<350> log likelihood: -662152\n",
      "INFO:lda:<360> log likelihood: -661899\n",
      "INFO:lda:<370> log likelihood: -661012\n",
      "INFO:lda:<380> log likelihood: -661278\n",
      "INFO:lda:<390> log likelihood: -661085\n",
      "INFO:lda:<400> log likelihood: -660418\n",
      "INFO:lda:<410> log likelihood: -660510\n",
      "INFO:lda:<420> log likelihood: -660343\n",
      "INFO:lda:<430> log likelihood: -659789\n",
      "INFO:lda:<440> log likelihood: -659336\n",
      "INFO:lda:<450> log likelihood: -659039\n",
      "INFO:lda:<460> log likelihood: -659329\n",
      "INFO:lda:<470> log likelihood: -658707\n",
      "INFO:lda:<480> log likelihood: -658879\n",
      "INFO:lda:<490> log likelihood: -658819\n",
      "INFO:lda:<500> log likelihood: -658407\n",
      "INFO:lda:<510> log likelihood: -658651\n",
      "INFO:lda:<520> log likelihood: -658111\n",
      "INFO:lda:<530> log likelihood: -658018\n",
      "INFO:lda:<540> log likelihood: -658111\n",
      "INFO:lda:<550> log likelihood: -657925\n",
      "INFO:lda:<560> log likelihood: -657860\n",
      "INFO:lda:<570> log likelihood: -657494\n",
      "INFO:lda:<580> log likelihood: -657723\n",
      "INFO:lda:<590> log likelihood: -657591\n",
      "INFO:lda:<600> log likelihood: -657557\n",
      "INFO:lda:<610> log likelihood: -657505\n",
      "INFO:lda:<620> log likelihood: -657730\n",
      "INFO:lda:<630> log likelihood: -657304\n",
      "INFO:lda:<640> log likelihood: -657208\n",
      "INFO:lda:<650> log likelihood: -657518\n",
      "INFO:lda:<660> log likelihood: -657541\n",
      "INFO:lda:<670> log likelihood: -657381\n",
      "INFO:lda:<680> log likelihood: -657575\n",
      "INFO:lda:<690> log likelihood: -656985\n",
      "INFO:lda:<700> log likelihood: -656815\n",
      "INFO:lda:<710> log likelihood: -656930\n",
      "INFO:lda:<720> log likelihood: -656538\n",
      "INFO:lda:<730> log likelihood: -656291\n",
      "INFO:lda:<740> log likelihood: -656417\n",
      "INFO:lda:<750> log likelihood: -656747\n",
      "INFO:lda:<760> log likelihood: -656600\n",
      "INFO:lda:<770> log likelihood: -656269\n",
      "INFO:lda:<780> log likelihood: -656311\n",
      "INFO:lda:<790> log likelihood: -656069\n",
      "INFO:lda:<800> log likelihood: -656228\n",
      "INFO:lda:<810> log likelihood: -656178\n",
      "INFO:lda:<820> log likelihood: -655694\n",
      "INFO:lda:<830> log likelihood: -655997\n",
      "INFO:lda:<840> log likelihood: -656224\n",
      "INFO:lda:<850> log likelihood: -656197\n",
      "INFO:lda:<860> log likelihood: -655889\n",
      "INFO:lda:<870> log likelihood: -656180\n",
      "INFO:lda:<880> log likelihood: -656997\n",
      "INFO:lda:<890> log likelihood: -655989\n",
      "INFO:lda:<900> log likelihood: -655615\n",
      "INFO:lda:<910> log likelihood: -655584\n",
      "INFO:lda:<920> log likelihood: -656602\n",
      "INFO:lda:<930> log likelihood: -656083\n",
      "INFO:lda:<940> log likelihood: -656294\n",
      "INFO:lda:<950> log likelihood: -656257\n",
      "INFO:lda:<960> log likelihood: -656243\n",
      "INFO:lda:<970> log likelihood: -656028\n",
      "INFO:lda:<980> log likelihood: -655603\n",
      "INFO:lda:<990> log likelihood: -656012\n",
      "INFO:lda:<1000> log likelihood: -655849\n",
      "INFO:lda:<1010> log likelihood: -655376\n",
      "INFO:lda:<1020> log likelihood: -655417\n",
      "INFO:lda:<1030> log likelihood: -655856\n",
      "INFO:lda:<1040> log likelihood: -655197\n",
      "INFO:lda:<1050> log likelihood: -655938\n",
      "INFO:lda:<1060> log likelihood: -655529\n",
      "INFO:lda:<1070> log likelihood: -655092\n",
      "INFO:lda:<1080> log likelihood: -655119\n",
      "INFO:lda:<1090> log likelihood: -656215\n",
      "INFO:lda:<1100> log likelihood: -655602\n",
      "INFO:lda:<1110> log likelihood: -655296\n",
      "INFO:lda:<1120> log likelihood: -655547\n",
      "INFO:lda:<1130> log likelihood: -655580\n",
      "INFO:lda:<1140> log likelihood: -655604\n",
      "INFO:lda:<1150> log likelihood: -655168\n",
      "INFO:lda:<1160> log likelihood: -655281\n",
      "INFO:lda:<1170> log likelihood: -655409\n",
      "INFO:lda:<1180> log likelihood: -655517\n",
      "INFO:lda:<1190> log likelihood: -654922\n",
      "INFO:lda:<1200> log likelihood: -655304\n",
      "INFO:lda:<1210> log likelihood: -655852\n",
      "INFO:lda:<1220> log likelihood: -655184\n",
      "INFO:lda:<1230> log likelihood: -655650\n",
      "INFO:lda:<1240> log likelihood: -655606\n",
      "INFO:lda:<1250> log likelihood: -656086\n",
      "INFO:lda:<1260> log likelihood: -655698\n",
      "INFO:lda:<1270> log likelihood: -655351\n",
      "INFO:lda:<1280> log likelihood: -655686\n",
      "INFO:lda:<1290> log likelihood: -654801\n",
      "INFO:lda:<1300> log likelihood: -654973\n",
      "INFO:lda:<1310> log likelihood: -655186\n",
      "INFO:lda:<1320> log likelihood: -655128\n",
      "INFO:lda:<1330> log likelihood: -655365\n",
      "INFO:lda:<1340> log likelihood: -655338\n",
      "INFO:lda:<1350> log likelihood: -655219\n",
      "INFO:lda:<1360> log likelihood: -655115\n",
      "INFO:lda:<1370> log likelihood: -654930\n",
      "INFO:lda:<1380> log likelihood: -655209\n",
      "INFO:lda:<1390> log likelihood: -654940\n",
      "INFO:lda:<1400> log likelihood: -655055\n",
      "INFO:lda:<1410> log likelihood: -655286\n",
      "INFO:lda:<1420> log likelihood: -655316\n",
      "INFO:lda:<1430> log likelihood: -655257\n",
      "INFO:lda:<1440> log likelihood: -654964\n",
      "INFO:lda:<1450> log likelihood: -654884\n",
      "INFO:lda:<1460> log likelihood: -655493\n",
      "INFO:lda:<1470> log likelihood: -655415\n",
      "INFO:lda:<1480> log likelihood: -655192\n",
      "INFO:lda:<1490> log likelihood: -655728\n",
      "INFO:lda:<1499> log likelihood: -655858\n"
     ]
    }
   ],
   "source": [
    "z = model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lda.lda.LDA"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(z)   ## Success to this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lda.lda.LDA at 0x2a29400b8d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Scikit-Learn 0.19.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, ..., 0, 0, 0],\n",
       "       [7, 0, 2, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 1, ..., 0, 0, 0],\n",
       "       [1, 0, 1, ..., 0, 0, 0],\n",
       "       [1, 0, 1, ..., 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_topics=20, max_iter=5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\anthony\\scripts\\nfl-topics\\venv\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "c:\\users\\anthony\\scripts\\nfl-topics\\venv\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7, learning_method=None,\n",
       "             learning_offset=10.0, max_doc_update_iter=100, max_iter=5,\n",
       "             mean_change_tol=0.001, n_components=10, n_jobs=1, n_topics=20,\n",
       "             perp_tol=0.1, random_state=0, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7, learning_method=None,\n",
       "             learning_offset=10.0, max_doc_update_iter=100, max_iter=5,\n",
       "             mean_change_tol=0.001, n_components=10, n_jobs=1, n_topics=20,\n",
       "             perp_tol=0.1, random_state=0, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "INFO:sklearn.datasets.twenty_newsgroups:Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n",
      "INFO:sklearn.datasets.twenty_newsgroups:Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "## Pipline Example\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "#Initialize Variables\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_topics = 10\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_samples = dataset.data[:n_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Setup a tf vectorizer\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95,\n",
    "                                min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(data_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_topics=20,\n",
    "                               max_iter=5,\n",
    "                               learning_method='online',\n",
    "                               learning_offset=50.,\n",
    "                               random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2000x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 51752 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\anthony\\scripts\\nfl-topics\\venv\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='online', learning_offset=50.0,\n",
       "             max_doc_update_iter=100, max_iter=5, mean_change_tol=0.001,\n",
       "             n_components=10, n_jobs=1, n_topics=20, perp_tol=0.1,\n",
       "             random_state=0, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.fit(tf)  ## Success to this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: NMF and LDA from scikit-learn docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_topics = 10\n",
    "n_top_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 2s.\n"
     ]
    }
   ],
   "source": [
    "# Load the 20 newsgroups dataset and vectorize it.  \n",
    "# Use a few heuristics to filter out useless terms early on, the posts are stripped of headers,\n",
    "# footers and quotes replies, and common english words, words occuring in only one document\n",
    "# or in at least 95% of the documents re removed.\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "data_samples = dataset.data[:n_samples]\n",
    "print(\"done in %0.fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features for NMF...\n",
      "done in 0.522s.\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Vectorization\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=.95,\n",
    "                                   min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featuresIdf = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(featuresIdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '10',\n",
       " '100',\n",
       " '11',\n",
       " '12',\n",
       " '128',\n",
       " '13',\n",
       " '130',\n",
       " '14',\n",
       " '15',\n",
       " '16',\n",
       " '17',\n",
       " '18',\n",
       " '19',\n",
       " '1992',\n",
       " '1993',\n",
       " '20',\n",
       " '200',\n",
       " '21',\n",
       " '22',\n",
       " '23',\n",
       " '24',\n",
       " '25',\n",
       " '250',\n",
       " '26',\n",
       " '27',\n",
       " '28',\n",
       " '29',\n",
       " '2nd',\n",
       " '30',\n",
       " '300',\n",
       " '31',\n",
       " '32',\n",
       " '33',\n",
       " '34',\n",
       " '35',\n",
       " '36',\n",
       " '37',\n",
       " '38',\n",
       " '3d',\n",
       " '40',\n",
       " '42',\n",
       " '43',\n",
       " '44',\n",
       " '45',\n",
       " '48',\n",
       " '49',\n",
       " '50',\n",
       " '500',\n",
       " '51',\n",
       " '55',\n",
       " '60',\n",
       " '66',\n",
       " '70',\n",
       " '72',\n",
       " '75',\n",
       " '80',\n",
       " '800',\n",
       " '86',\n",
       " '90',\n",
       " '92',\n",
       " '93',\n",
       " '__',\n",
       " 'able',\n",
       " 'ac',\n",
       " 'accept',\n",
       " 'access',\n",
       " 'according',\n",
       " 'act',\n",
       " 'action',\n",
       " 'actually',\n",
       " 'add',\n",
       " 'added',\n",
       " 'addition',\n",
       " 'address',\n",
       " 'administration',\n",
       " 'advance',\n",
       " 'age',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'aids',\n",
       " 'air',\n",
       " 'al',\n",
       " 'allow',\n",
       " 'allowed',\n",
       " 'alt',\n",
       " 'america',\n",
       " 'american',\n",
       " 'amiga',\n",
       " 'analysis',\n",
       " 'anonymous',\n",
       " 'answer',\n",
       " 'answers',\n",
       " 'anti',\n",
       " 'anybody',\n",
       " 'apartment',\n",
       " 'appears',\n",
       " 'apple',\n",
       " 'application',\n",
       " 'applications',\n",
       " 'apply',\n",
       " 'appreciated',\n",
       " 'approach',\n",
       " 'appropriate',\n",
       " 'apr',\n",
       " 'april',\n",
       " 'archive',\n",
       " 'area',\n",
       " 'areas',\n",
       " 'aren',\n",
       " 'argument',\n",
       " 'armenia',\n",
       " 'armenian',\n",
       " 'armenians',\n",
       " 'army',\n",
       " 'article',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'assume',\n",
       " 'atheism',\n",
       " 'attack',\n",
       " 'attacks',\n",
       " 'attempt',\n",
       " 'au',\n",
       " 'author',\n",
       " 'authority',\n",
       " 'available',\n",
       " 'average',\n",
       " 'away',\n",
       " 'azerbaijan',\n",
       " 'bad',\n",
       " 'based',\n",
       " 'basic',\n",
       " 'basically',\n",
       " 'begin',\n",
       " 'belief',\n",
       " 'believe',\n",
       " 'best',\n",
       " 'better',\n",
       " 'bible',\n",
       " 'big',\n",
       " 'bike',\n",
       " 'billion',\n",
       " 'bios',\n",
       " 'bit',\n",
       " 'black',\n",
       " 'block',\n",
       " 'blood',\n",
       " 'blue',\n",
       " 'board',\n",
       " 'bob',\n",
       " 'body',\n",
       " 'book',\n",
       " 'books',\n",
       " 'bought',\n",
       " 'box',\n",
       " 'brake',\n",
       " 'break',\n",
       " 'bring',\n",
       " 'btw',\n",
       " 'build',\n",
       " 'building',\n",
       " 'built',\n",
       " 'bus',\n",
       " 'business',\n",
       " 'buy',\n",
       " 'buying',\n",
       " 'ca',\n",
       " 'cable',\n",
       " 'called',\n",
       " 'calls',\n",
       " 'came',\n",
       " 'canada',\n",
       " 'car',\n",
       " 'card',\n",
       " 'cards',\n",
       " 'care',\n",
       " 'carry',\n",
       " 'cars',\n",
       " 'case',\n",
       " 'cases',\n",
       " 'cause',\n",
       " 'cc',\n",
       " 'cd',\n",
       " 'center',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'chance',\n",
       " 'change',\n",
       " 'changed',\n",
       " 'changes',\n",
       " 'cheap',\n",
       " 'check',\n",
       " 'children',\n",
       " 'chip',\n",
       " 'choice',\n",
       " 'christ',\n",
       " 'christian',\n",
       " 'christians',\n",
       " 'church',\n",
       " 'citizens',\n",
       " 'city',\n",
       " 'claim',\n",
       " 'clear',\n",
       " 'clearly',\n",
       " 'clinton',\n",
       " 'clipper',\n",
       " 'clock',\n",
       " 'close',\n",
       " 'code',\n",
       " 'cold',\n",
       " 'color',\n",
       " 'com',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'coming',\n",
       " 'command',\n",
       " 'comments',\n",
       " 'commercial',\n",
       " 'common',\n",
       " 'communications',\n",
       " 'community',\n",
       " 'comp',\n",
       " 'company',\n",
       " 'complete',\n",
       " 'completely',\n",
       " 'computer',\n",
       " 'condition',\n",
       " 'conference',\n",
       " 'congress',\n",
       " 'connector',\n",
       " 'consider',\n",
       " 'considered',\n",
       " 'contact',\n",
       " 'containing',\n",
       " 'contains',\n",
       " 'continue',\n",
       " 'control',\n",
       " 'controller',\n",
       " 'copies',\n",
       " 'copy',\n",
       " 'correct',\n",
       " 'cost',\n",
       " 'costs',\n",
       " 'couldn',\n",
       " 'countries',\n",
       " 'country',\n",
       " 'couple',\n",
       " 'course',\n",
       " 'court',\n",
       " 'cover',\n",
       " 'create',\n",
       " 'created',\n",
       " 'crime',\n",
       " 'crowd',\n",
       " 'cs',\n",
       " 'cubs',\n",
       " 'current',\n",
       " 'currently',\n",
       " 'cut',\n",
       " 'dangerous',\n",
       " 'data',\n",
       " 'database',\n",
       " 'date',\n",
       " 'dave',\n",
       " 'david',\n",
       " 'day',\n",
       " 'days',\n",
       " 'dc',\n",
       " 'dead',\n",
       " 'deal',\n",
       " 'death',\n",
       " 'dec',\n",
       " 'decided',\n",
       " 'defense',\n",
       " 'deleted',\n",
       " 'department',\n",
       " 'design',\n",
       " 'designed',\n",
       " 'details',\n",
       " 'developed',\n",
       " 'development',\n",
       " 'device',\n",
       " 'devices',\n",
       " 'did',\n",
       " 'didn',\n",
       " 'die',\n",
       " 'died',\n",
       " 'difference',\n",
       " 'different',\n",
       " 'difficult',\n",
       " 'digital',\n",
       " 'directly',\n",
       " 'directory',\n",
       " 'discussion',\n",
       " 'disease',\n",
       " 'disk',\n",
       " 'display',\n",
       " 'division',\n",
       " 'does',\n",
       " 'doesn',\n",
       " 'dog',\n",
       " 'doing',\n",
       " 'dollars',\n",
       " 'don',\n",
       " 'door',\n",
       " 'dos',\n",
       " 'dot',\n",
       " 'doubt',\n",
       " 'dr',\n",
       " 'drive',\n",
       " 'driver',\n",
       " 'drivers',\n",
       " 'drives',\n",
       " 'driving',\n",
       " 'drug',\n",
       " 'earlier',\n",
       " 'early',\n",
       " 'earth',\n",
       " 'easily',\n",
       " 'easy',\n",
       " 'edu',\n",
       " 'effect',\n",
       " 'effective',\n",
       " 'effort',\n",
       " 'email',\n",
       " 'encrypted',\n",
       " 'encryption',\n",
       " 'end',\n",
       " 'energy',\n",
       " 'enforcement',\n",
       " 'engine',\n",
       " 'entire',\n",
       " 'equipment',\n",
       " 'eric',\n",
       " 'error',\n",
       " 'especially',\n",
       " 'events',\n",
       " 'evidence',\n",
       " 'exactly',\n",
       " 'example',\n",
       " 'excellent',\n",
       " 'exist',\n",
       " 'expect',\n",
       " 'experience',\n",
       " 'explain',\n",
       " 'extra',\n",
       " 'face',\n",
       " 'fact',\n",
       " 'fair',\n",
       " 'fairly',\n",
       " 'faith',\n",
       " 'family',\n",
       " 'faq',\n",
       " 'far',\n",
       " 'fast',\n",
       " 'faster',\n",
       " 'father',\n",
       " 'fax',\n",
       " 'feature',\n",
       " 'features',\n",
       " 'federal',\n",
       " 'feel',\n",
       " 'field',\n",
       " 'figure',\n",
       " 'file',\n",
       " 'files',\n",
       " 'final',\n",
       " 'finally',\n",
       " 'fine',\n",
       " 'firearm',\n",
       " 'fit',\n",
       " 'floppy',\n",
       " 'flyers',\n",
       " 'folks',\n",
       " 'follow',\n",
       " 'following',\n",
       " 'food',\n",
       " 'force',\n",
       " 'forget',\n",
       " 'form',\n",
       " 'format',\n",
       " 'formats',\n",
       " 'free',\n",
       " 'freedom',\n",
       " 'friend',\n",
       " 'ftp',\n",
       " 'function',\n",
       " 'future',\n",
       " 'game',\n",
       " 'games',\n",
       " 'gas',\n",
       " 'gave',\n",
       " 'gay',\n",
       " 'general',\n",
       " 'generally',\n",
       " 'gets',\n",
       " 'getting',\n",
       " 'given',\n",
       " 'gives',\n",
       " 'giving',\n",
       " 'gm',\n",
       " 'goal',\n",
       " 'god',\n",
       " 'goes',\n",
       " 'going',\n",
       " 'gone',\n",
       " 'good',\n",
       " 'got',\n",
       " 'gov',\n",
       " 'government',\n",
       " 'graphics',\n",
       " 'great',\n",
       " 'greek',\n",
       " 'ground',\n",
       " 'group',\n",
       " 'groups',\n",
       " 'guess',\n",
       " 'gun',\n",
       " 'guns',\n",
       " 'guy',\n",
       " 'half',\n",
       " 'hand',\n",
       " 'happen',\n",
       " 'happened',\n",
       " 'happens',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'hardware',\n",
       " 'haven',\n",
       " 'having',\n",
       " 'head',\n",
       " 'heads',\n",
       " 'health',\n",
       " 'hear',\n",
       " 'heard',\n",
       " 'heart',\n",
       " 'heaven',\n",
       " 'held',\n",
       " 'hell',\n",
       " 'help',\n",
       " 'hi',\n",
       " 'high',\n",
       " 'higher',\n",
       " 'history',\n",
       " 'hit',\n",
       " 'hiv',\n",
       " 'hockey',\n",
       " 'hold',\n",
       " 'home',\n",
       " 'hope',\n",
       " 'hospital',\n",
       " 'hot',\n",
       " 'hours',\n",
       " 'house',\n",
       " 'hp',\n",
       " 'human',\n",
       " 'ibm',\n",
       " 'id',\n",
       " 'idea',\n",
       " 'ii',\n",
       " 'image',\n",
       " 'images',\n",
       " 'imagine',\n",
       " 'important',\n",
       " 'include',\n",
       " 'included',\n",
       " 'includes',\n",
       " 'including',\n",
       " 'increase',\n",
       " 'individual',\n",
       " 'info',\n",
       " 'information',\n",
       " 'input',\n",
       " 'inside',\n",
       " 'instead',\n",
       " 'insurance',\n",
       " 'interested',\n",
       " 'interesting',\n",
       " 'interface',\n",
       " 'internal',\n",
       " 'international',\n",
       " 'internet',\n",
       " 'involved',\n",
       " 'isn',\n",
       " 'israel',\n",
       " 'israeli',\n",
       " 'issue',\n",
       " 'james',\n",
       " 'jesus',\n",
       " 'jewish',\n",
       " 'jews',\n",
       " 'job',\n",
       " 'jobs',\n",
       " 'john',\n",
       " 'just',\n",
       " 'kept',\n",
       " 'key',\n",
       " 'keys',\n",
       " 'kill',\n",
       " 'killed',\n",
       " 'killing',\n",
       " 'kind',\n",
       " 'knew',\n",
       " 'know',\n",
       " 'knowledge',\n",
       " 'known',\n",
       " 'knows',\n",
       " 'lack',\n",
       " 'land',\n",
       " 'language',\n",
       " 'large',\n",
       " 'late',\n",
       " 'later',\n",
       " 'latest',\n",
       " 'launch',\n",
       " 'law',\n",
       " 'laws',\n",
       " 'leafs',\n",
       " 'learn',\n",
       " 'leave',\n",
       " 'led',\n",
       " 'left',\n",
       " 'legal',\n",
       " 'let',\n",
       " 'letter',\n",
       " 'level',\n",
       " 'library',\n",
       " 'license',\n",
       " 'life',\n",
       " 'light',\n",
       " 'like',\n",
       " 'likely',\n",
       " 'limited',\n",
       " 'line',\n",
       " 'lines',\n",
       " 'list',\n",
       " 'listen',\n",
       " 'little',\n",
       " 'live',\n",
       " 'lives',\n",
       " 'living',\n",
       " 'll',\n",
       " 'local',\n",
       " 'long',\n",
       " 'longer',\n",
       " 'look',\n",
       " 'looking',\n",
       " 'looks',\n",
       " 'lord',\n",
       " 'lost',\n",
       " 'lot',\n",
       " 'lots',\n",
       " 'love',\n",
       " 'low',\n",
       " 'luck',\n",
       " 'lunar',\n",
       " 'mac',\n",
       " 'machine',\n",
       " 'machines',\n",
       " 'magi',\n",
       " 'mail',\n",
       " 'main',\n",
       " 'major',\n",
       " 'make',\n",
       " 'makes',\n",
       " 'making',\n",
       " 'mamma',\n",
       " 'man',\n",
       " 'manager',\n",
       " 'manual',\n",
       " 'mark',\n",
       " 'market',\n",
       " 'marriage',\n",
       " 'mars',\n",
       " 'mary',\n",
       " 'mass',\n",
       " 'master',\n",
       " 'math',\n",
       " 'matter',\n",
       " 'maybe',\n",
       " 'mb',\n",
       " 'mean',\n",
       " 'meaning',\n",
       " 'means',\n",
       " 'media',\n",
       " 'medical',\n",
       " 'member',\n",
       " 'members',\n",
       " 'memory',\n",
       " 'men',\n",
       " 'mention',\n",
       " 'mentioned',\n",
       " 'message',\n",
       " 'middle',\n",
       " 'mike',\n",
       " 'mil',\n",
       " 'miles',\n",
       " 'military',\n",
       " 'million',\n",
       " 'mind',\n",
       " 'mission',\n",
       " 'mit',\n",
       " 'mode',\n",
       " 'model',\n",
       " 'models',\n",
       " 'modern',\n",
       " 'money',\n",
       " 'monitor',\n",
       " 'months',\n",
       " 'moon',\n",
       " 'moral',\n",
       " 'mother',\n",
       " 'motif',\n",
       " 'mr',\n",
       " 'ms',\n",
       " 'nasa',\n",
       " 'national',\n",
       " 'nature',\n",
       " 'navy',\n",
       " 'near',\n",
       " 'necessary',\n",
       " 'need',\n",
       " 'needed',\n",
       " 'needs',\n",
       " 'net',\n",
       " 'network',\n",
       " 'new',\n",
       " 'news',\n",
       " 'newsgroup',\n",
       " 'nhl',\n",
       " 'nice',\n",
       " 'night',\n",
       " 'non',\n",
       " 'normal',\n",
       " 'north',\n",
       " 'note',\n",
       " 'nsa',\n",
       " 'number',\n",
       " 'numbers',\n",
       " 'objects',\n",
       " 'obvious',\n",
       " 'offer',\n",
       " 'office',\n",
       " 'oh',\n",
       " 'oil',\n",
       " 'ok',\n",
       " 'old',\n",
       " 'older',\n",
       " 'ones',\n",
       " 'open',\n",
       " 'opinion',\n",
       " 'opinions',\n",
       " 'orbit',\n",
       " 'order',\n",
       " 'org',\n",
       " 'organization',\n",
       " 'original',\n",
       " 'os',\n",
       " 'output',\n",
       " 'outside',\n",
       " 'package',\n",
       " 'page',\n",
       " 'paper',\n",
       " 'papers',\n",
       " 'parents',\n",
       " 'particular',\n",
       " 'parts',\n",
       " 'party',\n",
       " 'pass',\n",
       " 'past',\n",
       " 'paul',\n",
       " 'pay',\n",
       " 'pc',\n",
       " 'people',\n",
       " 'performance',\n",
       " 'period',\n",
       " 'person',\n",
       " 'personal',\n",
       " 'peter',\n",
       " 'phone',\n",
       " 'pick',\n",
       " 'piece',\n",
       " 'pin',\n",
       " 'pittsburgh',\n",
       " 'place',\n",
       " 'places',\n",
       " 'plan',\n",
       " 'play',\n",
       " 'played',\n",
       " 'player',\n",
       " 'players',\n",
       " 'playing',\n",
       " 'plus',\n",
       " 'point',\n",
       " 'points',\n",
       " 'police',\n",
       " 'policy',\n",
       " 'political',\n",
       " 'population',\n",
       " 'port',\n",
       " 'position',\n",
       " 'possible',\n",
       " 'post',\n",
       " 'posted',\n",
       " 'posting',\n",
       " 'power',\n",
       " 'pp',\n",
       " 'present',\n",
       " 'president',\n",
       " 'press',\n",
       " 'pretty',\n",
       " 'price',\n",
       " 'printer',\n",
       " 'private',\n",
       " 'pro',\n",
       " 'probably',\n",
       " 'probe',\n",
       " 'probes',\n",
       " 'problem',\n",
       " 'problems',\n",
       " 'process',\n",
       " 'product',\n",
       " 'program',\n",
       " 'programs',\n",
       " 'project',\n",
       " 'prove',\n",
       " 'provide',\n",
       " 'pub',\n",
       " 'public',\n",
       " 'purpose',\n",
       " 'putting',\n",
       " 'quality',\n",
       " 'question',\n",
       " 'questions',\n",
       " 'quite',\n",
       " 'radio',\n",
       " 'ram',\n",
       " 'range',\n",
       " 'rate',\n",
       " 'rates',\n",
       " 'ray',\n",
       " 'read',\n",
       " 'reading',\n",
       " 'real',\n",
       " 'really',\n",
       " 'reason',\n",
       " 'reasonable',\n",
       " 'received',\n",
       " 'recent',\n",
       " 'recently',\n",
       " 'recommend',\n",
       " 'record',\n",
       " 'red',\n",
       " 'reference',\n",
       " 'regular',\n",
       " 'related',\n",
       " 'release',\n",
       " 'religion',\n",
       " 'religious',\n",
       " 'remember',\n",
       " 'reply',\n",
       " 'report',\n",
       " 'request',\n",
       " 'require',\n",
       " 'required',\n",
       " 'research',\n",
       " 'response',\n",
       " 'rest',\n",
       " 'result',\n",
       " 'results',\n",
       " 'return',\n",
       " 'right',\n",
       " 'rights',\n",
       " 'road',\n",
       " 'robert',\n",
       " 'role',\n",
       " 'rom',\n",
       " 'room',\n",
       " 'rules',\n",
       " 'run',\n",
       " 'running',\n",
       " 'runs',\n",
       " 'safety',\n",
       " 'said',\n",
       " 'sale',\n",
       " 'san',\n",
       " 'save',\n",
       " 'saw',\n",
       " 'say',\n",
       " 'saying',\n",
       " 'says',\n",
       " 'school',\n",
       " 'sci',\n",
       " 'science',\n",
       " 'scientific',\n",
       " 'screen',\n",
       " 'scsi',\n",
       " 'search',\n",
       " 'season',\n",
       " 'second',\n",
       " 'section',\n",
       " 'secure',\n",
       " 'security',\n",
       " 'seen',\n",
       " 'self',\n",
       " 'sell',\n",
       " 'send',\n",
       " 'sense',\n",
       " 'sent',\n",
       " 'serial',\n",
       " 'series',\n",
       " 'seriously',\n",
       " 'server',\n",
       " 'service',\n",
       " 'set',\n",
       " 'sex',\n",
       " 'sgi',\n",
       " 'shall',\n",
       " 'short',\n",
       " 'shot',\n",
       " 'shots',\n",
       " 'shows',\n",
       " 'shuttle',\n",
       " 'signal',\n",
       " 'similar',\n",
       " 'simple',\n",
       " 'simply',\n",
       " 'sin',\n",
       " 'single',\n",
       " 'site',\n",
       " 'situation',\n",
       " 'size',\n",
       " 'slow',\n",
       " 'small',\n",
       " 'society',\n",
       " 'software',\n",
       " 'solar',\n",
       " 'sold',\n",
       " 'soldiers',\n",
       " 'soon',\n",
       " 'sorry',\n",
       " 'sort',\n",
       " 'sound',\n",
       " 'sounds',\n",
       " 'source',\n",
       " 'sources',\n",
       " 'soviet',\n",
       " 'space',\n",
       " 'spacecraft',\n",
       " 'special',\n",
       " 'specific',\n",
       " 'specifically',\n",
       " 'speed',\n",
       " 'st',\n",
       " 'standard',\n",
       " 'start',\n",
       " 'started',\n",
       " 'starting',\n",
       " 'state',\n",
       " 'statement',\n",
       " 'states',\n",
       " 'station',\n",
       " 'stay',\n",
       " 'stop',\n",
       " 'story',\n",
       " 'street',\n",
       " 'strong',\n",
       " 'study',\n",
       " 'stuff',\n",
       " 'stupid',\n",
       " 'subject',\n",
       " 'suggest',\n",
       " 'sun',\n",
       " 'support',\n",
       " 'supported',\n",
       " 'supports',\n",
       " 'suppose',\n",
       " 'supposed',\n",
       " 'sure',\n",
       " 'surface',\n",
       " 'switch',\n",
       " 'systems',\n",
       " 'taken',\n",
       " 'takes',\n",
       " 'taking',\n",
       " 'talk',\n",
       " 'talking',\n",
       " 'tape',\n",
       " 'team',\n",
       " 'teams',\n",
       " 'technical',\n",
       " 'technology',\n",
       " 'tell',\n",
       " 'term',\n",
       " 'test',\n",
       " 'text',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'theory',\n",
       " 'thing',\n",
       " 'things',\n",
       " 'think',\n",
       " 'thinking',\n",
       " 'thought',\n",
       " 'time',\n",
       " 'times',\n",
       " 'tires',\n",
       " 'today',\n",
       " 'told',\n",
       " 'took',\n",
       " 'toronto',\n",
       " 'total',\n",
       " 'town',\n",
       " 'traffic',\n",
       " 'transfer',\n",
       " 'tried',\n",
       " 'trouble',\n",
       " 'true',\n",
       " 'trust',\n",
       " 'truth',\n",
       " 'try',\n",
       " 'trying',\n",
       " 'turkish',\n",
       " 'turn',\n",
       " 'turned',\n",
       " 'tv',\n",
       " 'type',\n",
       " 'types',\n",
       " 'uk',\n",
       " 'understand',\n",
       " 'unfortunately',\n",
       " 'united',\n",
       " 'university',\n",
       " 'unix',\n",
       " 'unless',\n",
       " 'use',\n",
       " 'used',\n",
       " 'useful',\n",
       " 'user',\n",
       " 'users',\n",
       " 'uses',\n",
       " 'using',\n",
       " 'usually',\n",
       " 'value',\n",
       " 'van',\n",
       " 'various',\n",
       " 've',\n",
       " 'version',\n",
       " 'vga',\n",
       " 'video',\n",
       " 'view',\n",
       " 'volume',\n",
       " 'vs',\n",
       " 'want',\n",
       " 'wanted',\n",
       " 'wants',\n",
       " 'war',\n",
       " 'washington',\n",
       " 'wasn',\n",
       " 'water',\n",
       " 'way',\n",
       " 'weapon',\n",
       " 'weapons',\n",
       " 'week',\n",
       " 'weeks',\n",
       " 'went',\n",
       " 'western',\n",
       " 'white',\n",
       " 'wife',\n",
       " 'willing',\n",
       " 'win',\n",
       " 'window',\n",
       " 'windows',\n",
       " 'wish',\n",
       " 'woman',\n",
       " 'women',\n",
       " 'won',\n",
       " 'wonder',\n",
       " 'wondering',\n",
       " 'word',\n",
       " 'words',\n",
       " 'work',\n",
       " 'worked',\n",
       " 'working',\n",
       " 'works',\n",
       " 'world',\n",
       " 'worse',\n",
       " 'worth',\n",
       " 'wouldn',\n",
       " 'write',\n",
       " 'written',\n",
       " 'wrong',\n",
       " 'xfree86',\n",
       " 'year',\n",
       " 'years',\n",
       " 'yes',\n",
       " 'young']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresIdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "done in 2.588s.\n"
     ]
    }
   ],
   "source": [
    "# TF Vectorization - raw term counts\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=.95,\n",
    "                                min_df=2,\n",
    "                                ngram_range=(1,3),\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print('done in %0.3fs.' % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 1000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 1000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The print top words function\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NMF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the NMF model with tf-idf freatures,  n_samples=2000 and n_features=1000...\n",
      "done in 0.407s.\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting the NMF model with tf-idf freatures,  \"\n",
    "      'n_samples=%d and n_features=%d...'\n",
    "      % (n_samples, n_features))\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "nmf = NMF(n_components=n_topics,\n",
    "          random_state=1,\n",
    "          alpha=.1,\n",
    "          l1_ratio=.5)\n",
    "\n",
    "nmfResult = nmf.fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 1000)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1000)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics in NMF model:\n",
      "Topic #0: just people don think like know time good make way\n",
      "Topic #1: windows use dos using window program os drivers application help\n",
      "Topic #2: god jesus bible faith christian christ christians does heaven sin\n",
      "Topic #3: thanks know does mail advance hi info interested email anybody\n",
      "Topic #4: car cars tires miles 00 new engine insurance price condition\n",
      "Topic #5: edu soon com send university internet mit ftp mail cc\n",
      "Topic #6: file problem files format win sound ftp pub read save\n",
      "Topic #7: game team games year win play season players nhl runs\n",
      "Topic #8: drive drives hard disk floppy software card mac computer power\n",
      "Topic #9: key chip clipper keys encryption government public use secure enforcement\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Topics in NMF model:\")\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NMF Model with generalized Kullback-Leibler divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the NMF model (generalized Kullback-Leibler divergence) with tf-idf freatures,  n_samples=2000 and n_features=1000...\n",
      "done in 2.481s.\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with tf-idf freatures,  \"\n",
    "      'n_samples=%d and n_features=%d...'\n",
    "      % (n_samples, n_features))\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "nmf = NMF(n_components=n_topics,\n",
    "          random_state=1,\n",
    "          beta_loss='kullback-leibler',\n",
    "          solver='mu',\n",
    "          max_iter=1000,\n",
    "          alpha=.1,\n",
    "          l1_ratio=.5)\n",
    "\n",
    "nmfResult = nmf.fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics in NMF model:\n",
      "Topic #0: people just like time don say really know way things\n",
      "Topic #1: windows thanks using help need hi work know use looking\n",
      "Topic #2: god does true read know say believe subject says religion\n",
      "Topic #3: thanks know like interested mail just want new send edu\n",
      "Topic #4: time new 10 year sale old offer 20 16 15\n",
      "Topic #5: use number com government new university data states information talk\n",
      "Topic #6: edu try file soon remember problem com program hope mike\n",
      "Topic #7: year world team game play won win games season maybe\n",
      "Topic #8: think don drive need hard make people mac read going\n",
      "Topic #9: just good use way got like ll doesn want sure\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Topics in NMF model:\")\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the LDA model  with tf freatures,  n_samples=2000 and n_features=1000...\n",
      "done in 7.212s.\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting the LDA model  with tf freatures,  \"\n",
    "      'n_samples=%d and n_features=%d...'\n",
    "      % (n_samples, n_features))\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=n_topics,\n",
    "                                max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0,\n",
    "                                )\n",
    "\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics in LDA model:\n",
      "Topic #0: space 10 earth 12 moon surface probe 93 00 lunar\n",
      "Topic #1: game scsi play team flyers season games cubs color points\n",
      "Topic #2: people just don think like know god time say make\n",
      "Topic #3: thanks know like does bike don help interested advance looking\n",
      "Topic #4: section military division win weapon firearm dangerous license shall application\n",
      "Topic #5: edu windows com file mail graphics version software send ftp\n",
      "Topic #6: just car like new good don year think better cars\n",
      "Topic #7: 55 11 10 18 15 17 25 26 22 23\n",
      "Topic #8: health hiv new national president aids information disease pittsburgh united\n",
      "Topic #9: drive disk card hard drives speed power controller hard disk rom\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Topics in LDA model:\")\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NFL-Topics",
   "language": "python",
   "name": "nfl-topics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
